# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Postgres â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
POSTGRES_USER=admin
POSTGRES_PASSWORD=change_me
POSTGRES_DB=n8n
POSTGRES_PORT=5432

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ n8n â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
N8N_HOST=localhost
N8N_PORT=5678
N8N_ENCRYPTION_KEY=change_me_to_a_long_random_string
N8N_USER_MANAGEMENT_JWT_SECRET=change_me_to_a_random_secret

# admin/owner (the one we POST in provision)
N8N_ADMIN_EMAIL=admin@example.com
N8N_ADMIN_FIRST_NAME=Admin
N8N_ADMIN_LAST_NAME=User
N8N_ADMIN_PASSWORD=change_me

# basic auth (MUST match provisioner)
N8N_BASIC_AUTH_USER=admin
N8N_BASIC_AUTH_PASSWORD=change_me

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ollama â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
OLLAMA_PORT=11434
OLLAMA_HOST=localhost

# To build vLLM locally (CPU), uncomment the line below:
# COMPOSE_FILE=docker-compose.yml:docker-compose.vllm-build.yml

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ vLLM â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
VLLM_PORT=8000
VLLM_IMAGE=public.ecr.aws/q9t5s3a7/vllm-cpu-release-repo:v0.7.2
VLLM_MODEL=huihui-ai/Huihui-Qwen3-VL-32B-Thinking-abliterated
HF_TOKEN=

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Langflow / Flowise / Open-WebUI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
LANGFLOW_PORT=7860
FLOWISE_PORT=3001
OPEN_WEBUI_PORT=3000

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Qdrant â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
QDRANT_PORT=6333


# Documentation MCP creds 
DOC_CLIENT_ID=
DOC_SECRET_KEY=
DOC_REGION=EU   # one of: EU, US, STG, Local


# SMS MCP Credentials
MANAGEMENT_HOST=
SMS_API_KEY=
TE_API_KEY=
REPUTATION_API_KEY=
SPARK_MGMT_CLIENT_ID=
SPARK_MGMT_SECRET_KEY=
SPARK_MGMT_REGION=US
SPARK_MGMT_INFINITY_PORTAL_URL=https://cloudinfra-gw-us.portal.checkpoint.com/auth/external
HARMONY_SASE_API_KEY=
HARMONY_SASE_MANAGEMENT_HOST=
HARMONY_SASE_REGION=

CPINFO_LOG_LEVEL=info

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Optional Services â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Enable optional services by setting these to true.
# COMPOSE_PROFILES must be set to 'true' for these toggles to work.
COMPOSE_PROFILES=true,cpu

# Deploy Flowise (LLM orchestration UI)
DEPLOY_FLOWISE=false

# Deploy Langflow (AI flow builder)
DEPLOY_LANGFLOW=false

# Deploy vLLM (OpenAI-compatible LLM server)
DEPLOY_VLLM=true

